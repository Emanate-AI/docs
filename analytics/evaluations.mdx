---
title: "Evaluations"
description: "Test scenarios for agent quality assurance"
---

# Evaluations

Evaluations let you create test scenarios to verify your agents behave correctly. Think of them as automated tests for your AI agents.

## Creating an Evaluation

Navigate to **Analytics** → **Evaluations** → **Create Eval**

### Step 1: Name Your Eval

Give it a descriptive name like "Lead Qualification Flow" or "Product FAQ Test".

### Step 2: Define Test Messages

Create a mock conversation with user messages:

```
User: "What products do you sell?"
User: "How much does the steel beam cost?"
User: "I'd like to place an order"
```

### Step 3: Set Evaluation Criteria

Choose how to judge the agent's response:

| Type | Use Case |
|------|----------|
| **Exact Match** | Response must contain specific text |
| **Regex** | Response matches a pattern |
| **AI Judge** | AI evaluates if response meets criteria |

### Step 4: Run the Eval

Select a target:
- **Assistant**: Test a single agent
- **Squad**: Test a workflow with multiple agents

## Evaluation Types

### Exact Match

Check if response contains specific content:

```
Criteria: "steel beam"
Pass if: Agent response contains "steel beam"
```

### Regex Match

Pattern-based validation:

```
Pattern: \$\d+(\.\d{2})?
Pass if: Agent mentions a price like "$125.00"
```

### AI Judge

Use AI to evaluate complex criteria:

```
Criteria: "Agent should politely decline to discuss 
competitor pricing and redirect to our products"
```

## Running Evaluations

### Manual Run

1. Open an eval
2. Select target agent or workflow
3. Click **Run**
4. View results

### Results

| Status | Meaning |
|--------|---------|
| **Pass** | All criteria met |
| **Fail** | One or more criteria failed |
| **Error** | Technical issue during test |

## Best Practices

### Test Key Scenarios

Create evals for:
- Common customer questions
- Lead qualification flows
- Edge cases and error handling
- Tool invocations

### Run After Changes

Re-run evals after updating:
- System prompts
- Knowledge base documents
- Tool configurations

### Start Simple

Begin with a few critical test cases:

```markdown
1. Basic greeting and response
2. Product information query
3. Lead capture flow
4. Transfer/escalation request
```

## Viewing Results

### Eval Detail Page

See for each run:
- Pass/fail status
- Full conversation transcript
- Which criteria passed or failed
- Timestamp and duration

### Stats Overview

Track across all evals:
- Total pass rate
- Recent run history
- Failing tests that need attention

## Troubleshooting

<AccordionGroup>
  <Accordion title="Eval keeps failing">
    - Review the full transcript to see what the agent said
    - Check if criteria are too strict
    - Verify agent has necessary knowledge/tools
  </Accordion>
  <Accordion title="Timeout errors">
    - Agent may be stuck in a loop
    - Check tool configurations
    - Simplify the test scenario
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Voice Agents" icon="phone" href="/agents/voice-agents">
    Configure agents
  </Card>
  <Card title="Testing" icon="flask" href="/agents/testing">
    Manual testing guide
  </Card>
  <Card title="Call Logs" icon="list" href="/analytics/call-logs">
    Review real conversations
  </Card>
  <Card title="Workflow Squads" icon="users" href="/agents/workflow-squads">
    Test workflows
  </Card>
</CardGroup>
